{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccfc80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Svetlana Toruno\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.30.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\svetlana toruno\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "import spacy\n",
    "import phonenumbers\n",
    "import re\n",
    "import uuid\n",
    "!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51ea50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dslim/bert-base-NER were not used when initializing TFBertForTokenClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dslim/bert-base-NER.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['R', 'E91247812', 'fa409901cfed4d1abab7f10ada1b93b3'], ['R', 'A12420291', '51bd8542353f49c4a72df9fac53d81bd'], ['R', 'J01417521', '9090a76915564fc199ea2976ed05e9d6'], ['R', 'O92942124', '62b580aac4684607ac6d5a35ed194b21'], ['R', 'K85129283', 'f058cf9fc1fe402e97ce21f3ab356f64'], ['N', 'Bill Burr', 'f415782e3e9d4d10914b125306b06d3f'], ['N', 'Jeffery', '3c166013d4b14090b03954282e85dd5d'], ['N', 'Amount', '7d3e09d09f274af59c8a4b10dc94432b'], ['N', 'Jacob Smith', '56384497006043b08b2d9dc9eb0b562c'], ['N', 'Blake Jones', 'fba96c653a114607be739e2f89a5bfc0'], ['N', 'Jill Withers', 'a656194b9ea1472991c937d1c580850e'], ['N', 'John Johnson', 'f24c2e36451f402aa657cec127d96b6c'], ['A', '1214 Apples Road', 'de8a1acca9794402a99b5a93a05883ae'], ['U', 'John Adams', 'fe2967468a474edeb1fe2069007ce5a8'], ['U', 'A12420291', '79ea134328bb4354ab40230109907ff4'], ['U', '251-51-2513', 'cbe88dc200194a92bb608a96396c16cf'], ['U', 'J01417521', '879666dcda954b82ae0327e3358497f9'], ['U', '824-12-6234', '88e1f31f7560490091b8d40b0cea5916'], ['U', 'O92942124', 'f2ab595efc674f098ca88f3f3ab90259'], ['U', '591-62-1241', '7946ac35c7544d3186d80cbc47d3a6e9'], ['U', 'K85129283', 'e7303468884c4b1ea65fffc6b93901a7'], ['U', '912-15-7475', '3b5abed7176743e8ada471339d342127'], ['U', '175819278', '0587edcb8507437ca74a92236d282a93'], ['U', 'J1323633', '642820a1b7704a9ea599bd639f4adf29'], ['U', 'E912848', '85dde8659bea4ff981dbea33b1261341'], ['U', '1241395133', 'bfb322777458464db38bb07512959591'], ['U', 'DL-9123471023', 'a233ba91c8f64f9bb30beac18aba064f']]\n"
     ]
    }
   ],
   "source": [
    "# N - Name \n",
    "# A - Address \n",
    "# O - Organization name \n",
    "# U - Undefined PII \n",
    "# P - Phone number \n",
    "# R - Passport\n",
    "# D - Driver's license\n",
    "# B - ID number\n",
    "# S - SSN\n",
    "# I - IP address \n",
    "# C - CC\n",
    "# E - Email\n",
    "\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "#model that worked the best with pdf \n",
    "#link to the hub that have banch of traned models, data sets, etc. if you want to check it out \n",
    "#https://huggingface.co/\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_model = \"dslim/bert-base-NER\"\n",
    "\n",
    "punc_list = '''!()[]{};*:'\"\\,<>./?_~-'''\n",
    "ssn_validate_pattern = \"^(?!666|000|9\\\\d{2})\\\\d{3}-(?!00)\\\\d{2}-(?!0{4})\\\\d{4}$\"\n",
    "email_validate_pattern = r\"^\\S+@\\S+\\.\\S+$\"\n",
    "ip_validate_pattern = \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"\n",
    "mac_validate_pattern = \"^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})|([0-9a-fA-F]{4}\\\\.[0-9a-fA-F]{4}\\\\.[0-9a-fA-F]{4})$\"\n",
    "passport_pattern_old = r\"^[A-Z0-9]{9}$\"\n",
    "passport_pattern_new = r\"^[A-Z]\\d{8}$\"\n",
    "# we can add more cc types\n",
    "amex_validate_pattern = \"^3[47][0-9]{13}$\"\n",
    "visa_validate_pattern = \"^4[0-9]{12}(?:[0-9]{3})?$\"\n",
    "master_card_validate_pattern = \"^(5[1-5][0-9]{14}|2(22[1-9][0-9]{12}|2[3-9][0-9]{13}|[3-6][0-9]{14}|7[0-1][0-9]{13}|720[0-9]{12}))$\"\n",
    "\n",
    "#read pdf and store it as a string \n",
    "p = open('Invoice  with lots of Passport Numbers, names, ssns, and drivers license numbers.pdf', 'rb')\n",
    "pdf = PyPDF2.PdfReader(p)\n",
    "text = pdf.pages[0].extract_text() + \"\\n\"\n",
    "text = \" \".join(text.replace(u\"\\xa0\", \" \").strip().split())\n",
    "text2 = text \n",
    "text4 = text \n",
    "\n",
    "#load text in nlp\n",
    "ner = pipeline(\"ner\", model=ner_model, grouped_entities=True)\n",
    "output = ner(text)\n",
    "\n",
    "#print(output)\n",
    "\n",
    "#print(\"Original string: \")\n",
    "#print(text)\n",
    "text3 = text\n",
    "\n",
    "#key words lists\n",
    "undetected_pii_list = [\"DL\", \"driver's license\", \"driving permit\", \"driver license\", \"drivers license\", \"dl#\",\"dls#\", \"lic#\",\"lics#\", \"ID\", \"passport\", \"bank account\", \"account number\", \"Passport number\"]\n",
    "passport_words = [\"passport number\" , \"passport#\"]\n",
    "dl_words = [\"DL\", \"driver's license\", \"driving permit\", \"driver license\", \"drivers license\", \"dl#\",\"dls#\", \"lic#\",\"lics#\", \"licenses\"]\n",
    "id_words = [\"ID\", \"ID#\", \"ID number\", \"identification documents\", \"ID card\"]\n",
    "\n",
    "#empty lists will be used to collect all the data associated with it and further stored in one nested loop to keep all the pii\n",
    "# in one place for easier tokenization\n",
    "people_list = []\n",
    "address_list = []\n",
    "ssn_list = []\n",
    "other_list = []\n",
    "email_list = []\n",
    "phone_list = []\n",
    "date_list = []\n",
    "undefined = []\n",
    "org_list = []\n",
    "ip_list = []\n",
    "cc_list = []\n",
    "passport_list = []\n",
    "all_pii = []\n",
    "\n",
    "#parts of the addresses sometimes wrongfully identified as names, so addresses needs to extracted first to avoid that \n",
    "for entity_group in output:\n",
    "    entity_label = entity_group[\"entity_group\"]  \n",
    "    if entity_label == \"LOC\":\n",
    "        temp = entity_group[\"word\"]\n",
    "        start_ind = text.find(temp)\n",
    "        end_ind = start_ind + len(temp)\n",
    "        if not text[end_ind].isspace() and text[end_ind] not in punc_list:\n",
    "            end_ind = text.find(\" \", end_ind)\n",
    "            temp = text[start_ind:end_ind]\n",
    "        \n",
    "        #print(\"Here is a substring \")\n",
    "        # Find the previous word and check if it is numeric to identify if it is part of the address \n",
    "        previous_word = text.rfind(\" \", 0, start_ind-1)\n",
    "        #print(previous_word)\n",
    "        # Extract the previous word using string slicing\n",
    "        p_word = text[previous_word + 1:start_ind-1]\n",
    "        isDigit = True;\n",
    "        for x in p_word:\n",
    "            if x.isdigit() == False:\n",
    "                isDigit == False;\n",
    "        # if it is numeric combine with the rest of address \n",
    "        if isDigit and len(p_word) < 5:\n",
    "            start_ind = previous_word\n",
    "            temp = p_word + \" \" + temp \n",
    "        address_list.append(temp)\n",
    "        #mask it with xxx for testing purposes \n",
    "        x = 'x';\n",
    "        for n in range (len(temp)-1):\n",
    "            x = x + 'x';             \n",
    "        text = text[:start_ind] + x + text[start_ind+len(x):]\n",
    "#print(text)\n",
    "\n",
    "# extract punctuation for better performance of the libraries \n",
    "for i in text4:\n",
    "    if i in punc_list:\n",
    "        if i == '.':\n",
    "            text4 = text4.replace(i, \" \")\n",
    "        else:\n",
    "            text4 = text4.replace(i, \"\")\n",
    "\n",
    "# detect passport number and replace it with xxx\n",
    "# only new passports have a format that unique from other \n",
    "for word in text4.split():\n",
    "    #edge case: make sure that there is no punctuation after the passport number; otherwise, it will not be detected\n",
    "    if word[-1] in punc_list:\n",
    "        word = word[:-1]\n",
    "    if (re.match(passport_pattern_new, word)):\n",
    "\n",
    "        passport_list.append(word)\n",
    "        x = 'x';\n",
    "        for n in range (len(word)-1):\n",
    "            x = x + 'x'; \n",
    "        text = text.replace(word, x)\n",
    "        \n",
    "new_list = []\n",
    "new_list.append(\"R\")\n",
    "new_list.append(passport_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "\n",
    "# load data from nlp into lists associated with it \n",
    "for entity_group in output:\n",
    "    \n",
    "    entity_label = entity_group[\"entity_group\"]  \n",
    "\n",
    "    if entity_label == \"PER\":\n",
    "        # check if the word is complete since the model sometimes tempts to grab just the beginning of the name \n",
    "        temp = entity_group[\"word\"]\n",
    "        start_ind = text.find(temp)\n",
    "        end_ind = start_ind + len(temp)\n",
    "        if not text[end_ind].isspace() and text[end_ind] not in punc_list:\n",
    "            end_ind = text.find(\" \", end_ind)\n",
    "            temp = text[start_ind:end_ind]\n",
    "        people_list.append(temp)\n",
    "        #mask it with xxx for testing purposes \n",
    "        x = 'x';\n",
    "        for n in range (len(temp)-1):\n",
    "            x = x + 'x';             \n",
    "        text = text[:start_ind] + x + text[start_ind+len(x):]\n",
    "    elif entity_label == \"MISC\":\n",
    "        # check if the word is complete since the model sometimes tempts to grab just the beginning of the name \n",
    "        temp = entity_group[\"word\"]\n",
    "        start_ind = text.find(temp)\n",
    "        end_ind = start_ind + len(temp)\n",
    "        if not text[end_ind].isspace() and text[end_ind] not in punc_list:\n",
    "            end_ind = text.find(\" \", end_ind)\n",
    "            temp = text[start_ind:end_ind]\n",
    "        undefined.append(temp)\n",
    "         #mask it with xxx for testing purposes \n",
    "        x = 'x';\n",
    "        for n in range (len(temp)-1):\n",
    "            x = x + 'x';             \n",
    "        text = text[:start_ind] + x + text[start_ind+len(x):]\n",
    "    elif entity_label == \"ORG\":\n",
    "        org_list.append(entity_group[\"word\"])\n",
    "\n",
    "        \n",
    "\n",
    "# create temporary new lists that will hold the char associated with pii and a list of all piis \n",
    "# then append each list in one nested list all_pii \n",
    "# so we end up getting something like this:\n",
    "# all_pii = [['N', ['Chris Johnson']], ['O', ['Diamond Star International', 'Diamond Star']], ['P', ['713-832-1234']]]\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"N\")\n",
    "new_list.append(people_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"A\")\n",
    "new_list.append(address_list)      \n",
    "all_pii.append(new_list)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"O\")\n",
    "new_list.append(org_list)      \n",
    "all_pii.append(new_list)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"U\")\n",
    "new_list.append(undefined)      \n",
    "all_pii.append(new_list)\n",
    "\n",
    "# mask all the pii detected by nlp with xxx for testing purposes \n",
    "for entity_group in output:\n",
    "   \n",
    "    if text.find(entity_group[\"word\"]) > -1:\n",
    "        temp = entity_group[\"word\"]\n",
    "        for word in temp.split():       \n",
    "            if text.find(word) > -1:\n",
    "                index = text.find(word)\n",
    "                x = 'x';\n",
    "                for n in range (len(word)-1):\n",
    "                    x = x + 'x';             \n",
    "                text = text[:index] + x + text[index+len(x):]\n",
    "            \n",
    "\n",
    "# The code below will handle all the pii that NLP did not detect\n",
    "#--------------------------------------------------------------------\n",
    "# Extract sentences that have keywords and insert them into new_list \n",
    "\n",
    "new_list = []\n",
    "tok_text = tokenize.sent_tokenize(text2)\n",
    "for sentence in tok_text:\n",
    "    for word in undetected_pii_list:        \n",
    "        if word in sentence:\n",
    "            if sentence not in new_list:\n",
    "                new_list.append(sentence)\n",
    "\n",
    "# get rid of the sentences that have keywords but don't have digits \n",
    "for sentence in new_list:\n",
    "    isDigit = False;\n",
    "    for word in sentence: \n",
    "        if word.isdigit() == True:\n",
    "            isDigit = True;\n",
    "    if isDigit == False:\n",
    "        new_list.remove(sentence)\n",
    "\n",
    "#create a nested list that will hold char of the pii and the value\n",
    "clean_undetected_list = [] \n",
    "\n",
    "for sentence in new_list:\n",
    "    #print(sentence)\n",
    "    sentence_list = []\n",
    "    #check if there is only one type of the document mentioned in the sentence \n",
    "    #if there id more than one, the pii will go into undefined list\n",
    "    one_type = 0;\n",
    "    # assign char depending on the type of pii\n",
    "    for word in id_words:\n",
    "        if word.lower() in sentence.lower():\n",
    "            one_type = one_type + 1\n",
    "            char = \"B\"\n",
    "    for word in dl_words:\n",
    "        if word.lower() in sentence.lower():\n",
    "            one_type = one_type + 1\n",
    "            char = \"D\"\n",
    "    #trace the the numeric pii \n",
    "    for word in sentence.split():\n",
    "        isDigit = False;\n",
    "        for char in word:\n",
    "            if char.isdigit() == True:\n",
    "                isDigit = True; \n",
    "        if isDigit == True and len(word) > 5 and word[0] != \"$\":\n",
    "            # sometimes it will grab pii with punctuation, so we need to make sure to get rid of it before passing into the list \n",
    "            if word[-1] in punc_list:\n",
    "                word = word[:-1]\n",
    "            if one_type == 1:\n",
    "                clean_undetected_list.append(char)\n",
    "                sentence_list.append(word)\n",
    "            else:\n",
    "                undefined.append(word)\n",
    "            #replace pii with xxx\n",
    "            if text.find(word) > -1:\n",
    "                index = text.find(word)\n",
    "                x = 'x';\n",
    "                for n in range (len(word)-1):\n",
    "                    x = x + 'x';             \n",
    "                text = text[:index] + x + text[index+len(x):]\n",
    "    if len(sentence_list) > 0 :\n",
    "        clean_undetected_list.append(sentence_list)\n",
    "#if len(clean_undetected_list) > 0:\n",
    "    #all_pii.append(clean_undetected_list)\n",
    "\n",
    "# detect ssn and replace it with xxx\n",
    "for word in text.split():\n",
    "    #edge case: make sure that there is no punctuation after the ssn; otherwise, it will not be detected\n",
    "    #all the punctuation can not be extracted at this point because \"-\" are part of the ssn\n",
    "    if word[-1] in punc_list:\n",
    "        word = word[:-1]\n",
    "    if (re.match(ssn_validate_pattern, word)):\n",
    "        # append ssn to ssn_list\n",
    "        ssn_list.append(word)\n",
    "        x = 'x';\n",
    "        for n in range (len(word)-1):\n",
    "            x = x + 'x'; \n",
    "        text = text.replace(word, x)\n",
    "        \n",
    "new_list = []\n",
    "new_list.append(\"S\")\n",
    "new_list.append(ssn_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "# detect phone numbers and replace them with xxx       \n",
    "for match in phonenumbers.PhoneNumberMatcher(text, \"US\"):\n",
    "    for word in text.split():\n",
    "        if word == match.raw_string:\n",
    "        #append phone numbers to phone_list\n",
    "            phone_list.append(word)\n",
    "            x = 'x';\n",
    "            for n in range (len(word)-1):\n",
    "                x = x + 'x'; \n",
    "            text = text.replace(word, x)\n",
    "            \n",
    "new_list = []\n",
    "new_list.append(\"P\")\n",
    "new_list.append(phone_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "# detect email and replace it with xxx\n",
    "for word in text.split():\n",
    "    if (re.match(email_validate_pattern, word)):\n",
    "        #edge case: make sure that there is no punctuation after the email; otherwise, it will be grabbed and considered as a part of the email\n",
    "        #all the punctuation can not be extracted at this point because \"@\" and \".\" are part of the email\n",
    "        if word[-1] in punc_list:\n",
    "            word = word[:-1]\n",
    "        email_list.append(word)\n",
    "        x = 'x';\n",
    "        for n in range (len(word)-1):\n",
    "            x = x + 'x'; \n",
    "            \n",
    "        text = text.replace(word, x)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"E\")\n",
    "new_list.append(email_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "# detect ip address and replace it with xxx\n",
    "for word in text.split():\n",
    "    if (re.match(ip_validate_pattern, word) or re.match(mac_validate_pattern, word)):\n",
    "        #edge case: make sure that there is no punctuation after the ip address; otherwise, it will be grabbed and considered as a part of the address\n",
    "        #all the punctuation can not be extracted at this point because \".\" is part of the address \n",
    "        if word[-1] in punc_list:\n",
    "            word = word[:-1]\n",
    "        ip_list.append(word)\n",
    "        x = 'x';\n",
    "        for n in range (len(word)-1):\n",
    "            x = x + 'x'; \n",
    "            \n",
    "        text = text.replace(word, x)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"I\")\n",
    "new_list.append(ip_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "# detect cc and replace it with xxx\n",
    "for word in text.split():\n",
    "    if (re.match(amex_validate_pattern, word) or re.match(visa_validate_pattern, word) or re.match(master_card_validate_pattern, word)):\n",
    "        #edge case: make sure that there is no punctuation after the cc\n",
    "        if word[-1] in punc_list:\n",
    "            word = word[:-1]\n",
    "        cc_list.append(word)\n",
    "        x = 'x';\n",
    "        for n in range (len(word)-1):\n",
    "            x = x + 'x'; \n",
    "            \n",
    "        text = text.replace(word, x)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"C\")\n",
    "new_list.append(cc_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "#second nlp that we are using for dates \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)            \n",
    " \n",
    "# using spacy library to detect dates\n",
    "for entity in doc.ents:\n",
    "    #print(entity.label_ +\": \" + entity.text)\n",
    "    if entity.label_ == \"DATE\":\n",
    "        temp = entity.text\n",
    "        for punct in punc_list:\n",
    "            if punct in temp:\n",
    "                temp = temp.replace(punct, \" \")\n",
    "        date_list.append(temp)\n",
    "        for word in temp.split():       \n",
    "            if text.find(word) > -1:\n",
    "                index = text.find(word)\n",
    "                x = 'x';\n",
    "                for n in range (len(word)-1):\n",
    "                    x = x + 'x';             \n",
    "                text = text[:index] + x + text[index+len(x):]\n",
    "#print()\n",
    "#print(\"String with masked pii: \")\n",
    "#print(text)\n",
    "new_list = []\n",
    "new_list.append(\"D\")\n",
    "#new_list.append(date_list)\n",
    "\n",
    "# We have to figure out what to do with dates; we will probably just have to manually check if they match the format of the DOB because we don't \n",
    "# need to tokenize all the dates \n",
    "\n",
    "#all_pii.append(new_list)\n",
    "#print(all_pii)\n",
    "\n",
    "token_list = []\n",
    "\n",
    "#sub the pii with uuid to pass it to chat gpt\n",
    "for pii_list in all_pii:\n",
    "    if len(pii_list[1]) != 0:\n",
    "        char = pii_list[0][0]\n",
    "        for element in pii_list[1]:\n",
    "            temp = []\n",
    "            temp.append(char)\n",
    "            temp.append(element)\n",
    "            index = text3.find(element)\n",
    "            str = uuid.uuid4() \n",
    "            temp.append(str.hex)\n",
    "            text3 = text3[:index] + char + \"-\" + str.hex + text3[index+len(element):]\n",
    "            token_list.append(temp)\n",
    "            \n",
    "    \n",
    "print(token_list)            \n",
    "#print(\"String with UUID pii: \")\n",
    "#print(text3)\n",
    "\n",
    "\n",
    "    # TODO: Write to file\n",
    "\n",
    "def replace(text):\n",
    "    # new_text = replace_with_pii(text)\n",
    "    # write to file \n",
    "    pass\n",
    "\n",
    "#Database \n",
    "\n",
    "load_dotenv()\n",
    "SNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "\n",
    "con_def = snowflake.connector.connect(user='BRENDANMORONEY',\n",
    "                                     account='ydpcciy-xn91624',\n",
    "                                     password =SNOWFLAKE_PASSWORD,\n",
    "                                     database='PII_TOKENIZATION',        \n",
    "                                     schema ='PUBLIC',\n",
    "                                     autocommit=True)         \n",
    "\n",
    "db_cursor_def = con_def.cursor()\n",
    "\n",
    "db_cursor_def.execute(\"CREATE WAREHOUSE IF NOT EXISTS pii_warehouse\")\n",
    "db_cursor_def.execute(\"USE WAREHOUSE pii_warehouse\")\n",
    "\n",
    "db_cursor_def.execute(\"CREATE DATABASE IF NOT EXISTS PII_TOKENIZATION\")\n",
    "db_cursor_def.execute(\"USE DATABASE PII_TOKENIZATION\")\n",
    "\n",
    "db_cursor_def.execute(\"CREATE SCHEMA IF NOT EXISTS PUBLIC\")\n",
    "db_cursor_def.execute(\"USE SCHEMA PII_TOKENIZATION.PUBLIC\")\n",
    "\n",
    "#Creates table with PII_value, PII_type and ID\n",
    "\n",
    "db_cursor_def.execute(\"\"\"CREATE OR REPLACE TABLE \n",
    "PII_TOKENIZATION.PUBLIC.PII_Token_XREF (Token TEXT, PII_VALUE \n",
    "VARCHAR(16777216),PII_TYPE VARCHAR(16777216), rec_created_date TIMESTAMP, \n",
    "user_added TEXT, updated_date TIMESTAMP, PRIMARY KEY(Token))\"\"\")\n",
    "\n",
    "#Creates log table\n",
    "\n",
    "db_cursor_def.execute(\"\"\"CREATE OR REPLACE TABLE PII_TOKENIZATION.PUBLIC.log \n",
    "(time TIMESTAMP, user TEXT, document TEXT, PII_type TEXT, override \n",
    "boolean)\"\"\")\n",
    "                      \n",
    "#Practice insert\n",
    "#TODO when PII is matched to Token, add functionality to store and remove from database\n",
    "\n",
    "db_cursor_def.execute(\"INSERT INTO PII_Token_XREF(Token, PII_VALUE, PII_TYPE) VALUES('c2783f59-743e-403c-beac-21cb67076292','Rick Owens', 'N')\")\n",
    "\n",
    "db_cursor_def.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = sys.argv()[1]\n",
    "    remove(text) if sys.argv()[2] == \"1\" else replace(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a00d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47266e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
