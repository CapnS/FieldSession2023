{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ccfc80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/brendanmoroney/anaconda3/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "import spacy\n",
    "import phonenumbers\n",
    "import re\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51ea50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dslim/bert-base-NER were not used when initializing TFBertForTokenClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dslim/bert-base-NER.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['R', 'E91247812', '43a1ad1039684da0ba339db966fc2084'], ['R', 'A12420291', 'e879e2f607cf4f658cb692cad3848f25'], ['R', 'J01417521', '4665eff5db594d46ae92921e88954970'], ['R', 'O92942124', '05abd847f78d44f394dc7b75c746eebd'], ['R', 'K85129283', '9929bbcdf0b34b8cb1537399cf50947e'], ['N', 'Bill Burr', '4e2aa75b96d743bea8619707d9ddedd0'], ['N', 'Jeffery', 'ebfa655e18b34562ac4d98c7de89904a'], ['N', 'Amount', 'e8299877e1fd42c38a07de24015f5ec9'], ['N', 'Jacob Smith', '05e4617f5fad4d8788e18ee172deba32'], ['N', 'Blake Jones', 'f844da095b4f490d97c05bf67c994d16'], ['N', 'Jill Withers', 'f64f29be5cec4a099fa9d53a4d0e4932'], ['N', 'John Johnson', '1e47388ac2a44305bfcc1ae3413812df'], ['A', '1214 Apples Road', 'e1056e99222a40c78902c7e5f3afc14d'], ['U', 'John Adams', 'ada70758d50f4749a3e807d77959e184'], ['U', 'A12420291', '6008d87f9ede4506a9a6f2dc38190ed5'], ['U', '251-51-2513', '474dcad0a6dc43a8b23ac0c58e64e821'], ['U', 'J01417521', '2ec8633d66024615a7c8877292cf5e26'], ['U', '824-12-6234', '59f5ad65debd43029c88435d3f36898a'], ['U', 'O92942124', '6a9135ec88e849f88ca3419dcac14d08'], ['U', '591-62-1241', '6269ca93dbdf4240a2be1062498fcc36'], ['U', 'K85129283', '86e89c4014bb43efb9af9ef3ae745952'], ['U', '912-15-7475', '994120038dcf454e9b27b559fe7faa26'], ['U', '175819278', '3592b3f33cf245f4a4d0e4e11d5c141c'], ['U', 'J1323633', '55c8a7f50d35422ab5adbd17bbf5d29c'], ['U', 'E912848', 'e79b579a7d1b4ea584e883cda02c2f3c'], ['U', '1241395133', 'ed146cb108244c48b451a980ec57b0b2'], ['U', 'DL-9123471023', 'b5363f2b638648d797942cc18e029517']]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'snowflake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 438\u001b[0m\n\u001b[1;32m    435\u001b[0m load_dotenv()\n\u001b[1;32m    436\u001b[0m SNOWFLAKE_PASSWORD \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mSNOWFLAKE_PASSWORD\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 438\u001b[0m con_def \u001b[39m=\u001b[39m snowflake\u001b[39m.\u001b[39mconnector\u001b[39m.\u001b[39mconnect(user\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBRENDANMORONEY\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    439\u001b[0m                                      account\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mydpcciy-xn91624\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    440\u001b[0m                                      password \u001b[39m=\u001b[39mSNOWFLAKE_PASSWORD,\n\u001b[1;32m    441\u001b[0m                                      database\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPII_TOKENIZATION\u001b[39m\u001b[39m'\u001b[39m,        \n\u001b[1;32m    442\u001b[0m                                      schema \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPUBLIC\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    443\u001b[0m                                      autocommit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)         \n\u001b[1;32m    445\u001b[0m db_cursor_def \u001b[39m=\u001b[39m con_def\u001b[39m.\u001b[39mcursor()\n\u001b[1;32m    447\u001b[0m db_cursor_def\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mCREATE WAREHOUSE IF NOT EXISTS pii_warehouse\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'snowflake' is not defined"
     ]
    }
   ],
   "source": [
    "# N - Name \n",
    "# A - Address \n",
    "# O - Organization name \n",
    "# U - Undefined PII \n",
    "# P - Phone number \n",
    "# R - Passport\n",
    "# D - Driver's license\n",
    "# B - ID number\n",
    "# S - SSN\n",
    "# I - IP address \n",
    "# C - CC\n",
    "# E - Email\n",
    "\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "#model that worked the best with pdf \n",
    "#link to the hub that have banch of traned models, data sets, etc. if you want to check it out \n",
    "#https://huggingface.co/\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_model = \"dslim/bert-base-NER\"\n",
    "\n",
    "punc_list = '''!()[]{};*:'\"\\,<>./?_~-'''\n",
    "ssn_validate_pattern = \"^(?!666|000|9\\\\d{2})\\\\d{3}-(?!00)\\\\d{2}-(?!0{4})\\\\d{4}$\"\n",
    "email_validate_pattern = r\"^\\S+@\\S+\\.\\S+$\"\n",
    "ip_validate_pattern = \"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\"\n",
    "mac_validate_pattern = \"^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})|([0-9a-fA-F]{4}\\\\.[0-9a-fA-F]{4}\\\\.[0-9a-fA-F]{4})$\"\n",
    "passport_pattern_old = r\"^[A-Z0-9]{9}$\"\n",
    "passport_pattern_new = r\"^[A-Z]\\d{8}$\"\n",
    "# we can add more cc types\n",
    "amex_validate_pattern = \"^3[47][0-9]{13}$\"\n",
    "visa_validate_pattern = \"^4[0-9]{12}(?:[0-9]{3})?$\"\n",
    "master_card_validate_pattern = \"^(5[1-5][0-9]{14}|2(22[1-9][0-9]{12}|2[3-9][0-9]{13}|[3-6][0-9]{14}|7[0-1][0-9]{13}|720[0-9]{12}))$\"\n",
    "\n",
    "#read pdf and store it as a string \n",
    "p = open('Invoice  with lots of Passport Numbers, names, ssns, and drivers license numbers.pdf', 'rb')\n",
    "pdf = PyPDF2.PdfReader(p)\n",
    "text = pdf.pages[0].extract_text() + \"\\n\"\n",
    "text = \" \".join(text.replace(u\"\\xa0\", \" \").strip().split())\n",
    "text2 = text \n",
    "text4 = text \n",
    "\n",
    "#load text in nlp\n",
    "ner = pipeline(\"ner\", model=ner_model, grouped_entities=True)\n",
    "output = ner(text)\n",
    "\n",
    "#print(output)\n",
    "\n",
    "#print(\"Original string: \")\n",
    "#print(text)\n",
    "text3 = text\n",
    "\n",
    "#key words lists\n",
    "undetected_pii_list = [\"DL\", \"driver's license\", \"driving permit\", \"driver license\", \"drivers license\", \"dl#\",\"dls#\", \"lic#\",\"lics#\", \"ID\", \"passport\", \"bank account\", \"account number\", \"Passport number\"]\n",
    "passport_words = [\"passport number\" , \"passport#\"]\n",
    "dl_words = [\"DL\", \"driver's license\", \"driving permit\", \"driver license\", \"drivers license\", \"dl#\",\"dls#\", \"lic#\",\"lics#\", \"licenses\"]\n",
    "id_words = [\"ID\", \"ID#\", \"ID number\", \"identification documents\", \"ID card\"]\n",
    "\n",
    "#empty lists will be used to collect all the data associated with it and further stored in one nested loop to keep all the pii\n",
    "# in one place for easier tokenization\n",
    "people_list = []\n",
    "address_list = []\n",
    "ssn_list = []\n",
    "other_list = []\n",
    "email_list = []\n",
    "phone_list = []\n",
    "date_list = []\n",
    "undefined = []\n",
    "org_list = []\n",
    "ip_list = []\n",
    "cc_list = []\n",
    "passport_list = []\n",
    "all_pii = []\n",
    "\n",
    "#parts of the addresses sometimes wrongfully identified as names, so addresses needs to extracted first to avoid that \n",
    "for entity_group in output:\n",
    "    entity_label = entity_group[\"entity_group\"]  \n",
    "    if entity_label == \"LOC\":\n",
    "        temp = entity_group[\"word\"]\n",
    "        start_ind = text.find(temp)\n",
    "        end_ind = start_ind + len(temp)\n",
    "        if not text[end_ind].isspace() and text[end_ind] not in punc_list:\n",
    "            end_ind = text.find(\" \", end_ind)\n",
    "            temp = text[start_ind:end_ind]\n",
    "        \n",
    "        #print(\"Here is a substring \")\n",
    "        # Find the previous word and check if it is numeric to identify if it is part of the address \n",
    "        previous_word = text.rfind(\" \", 0, start_ind-1)\n",
    "        #print(previous_word)\n",
    "        # Extract the previous word using string slicing\n",
    "        p_word = text[previous_word + 1:start_ind-1]\n",
    "        isDigit = True;\n",
    "        for x in p_word:\n",
    "            if x.isdigit() == False:\n",
    "                isDigit == False;\n",
    "        # if it is numeric combine with the rest of address \n",
    "        if isDigit and len(p_word) < 5:\n",
    "            start_ind = previous_word\n",
    "            temp = p_word + \" \" + temp \n",
    "        address_list.append(temp)\n",
    "        #mask it with xxx for testing purposes \n",
    "        x = 'x';\n",
    "        for n in range (len(temp)-1):\n",
    "            x = x + 'x';             \n",
    "        text = text[:start_ind] + x + text[start_ind+len(x):]\n",
    "#print(text)\n",
    "\n",
    "# extract punctuation for better performance of the libraries \n",
    "for i in text4:\n",
    "    if i in punc_list:\n",
    "        if i == '.':\n",
    "            text4 = text4.replace(i, \" \")\n",
    "        else:\n",
    "            text4 = text4.replace(i, \"\")\n",
    "\n",
    "# detect passport number and replace it with xxx\n",
    "# only new passports have a format that unique from other \n",
    "for word in text4.split():\n",
    "    #edge case: make sure that there is no punctuation after the passport number; otherwise, it will not be detected\n",
    "    if word[-1] in punc_list:\n",
    "        word = word[:-1]\n",
    "    if (re.match(passport_pattern_new, word)):\n",
    "\n",
    "        passport_list.append(word)\n",
    "        x = 'x';\n",
    "        for n in range (len(word)-1):\n",
    "            x = x + 'x'; \n",
    "        text = text.replace(word, x)\n",
    "        \n",
    "new_list = []\n",
    "new_list.append(\"R\")\n",
    "new_list.append(passport_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "\n",
    "# load data from nlp into lists associated with it \n",
    "for entity_group in output:\n",
    "    \n",
    "    entity_label = entity_group[\"entity_group\"]  \n",
    "\n",
    "    if entity_label == \"PER\":\n",
    "        # check if the word is complete since the model sometimes tempts to grab just the beginning of the name \n",
    "        temp = entity_group[\"word\"]\n",
    "        start_ind = text.find(temp)\n",
    "        end_ind = start_ind + len(temp)\n",
    "        if not text[end_ind].isspace() and text[end_ind] not in punc_list:\n",
    "            end_ind = text.find(\" \", end_ind)\n",
    "            temp = text[start_ind:end_ind]\n",
    "        people_list.append(temp)\n",
    "        #mask it with xxx for testing purposes \n",
    "        x = 'x';\n",
    "        for n in range (len(temp)-1):\n",
    "            x = x + 'x';             \n",
    "        text = text[:start_ind] + x + text[start_ind+len(x):]\n",
    "    elif entity_label == \"MISC\":\n",
    "        # check if the word is complete since the model sometimes tempts to grab just the beginning of the name \n",
    "        temp = entity_group[\"word\"]\n",
    "        start_ind = text.find(temp)\n",
    "        end_ind = start_ind + len(temp)\n",
    "        if not text[end_ind].isspace() and text[end_ind] not in punc_list:\n",
    "            end_ind = text.find(\" \", end_ind)\n",
    "            temp = text[start_ind:end_ind]\n",
    "        undefined.append(temp)\n",
    "         #mask it with xxx for testing purposes \n",
    "        x = 'x';\n",
    "        for n in range (len(temp)-1):\n",
    "            x = x + 'x';             \n",
    "        text = text[:start_ind] + x + text[start_ind+len(x):]\n",
    "    elif entity_label == \"ORG\":\n",
    "        org_list.append(entity_group[\"word\"])\n",
    "\n",
    "        \n",
    "\n",
    "# create temporary new lists that will hold the char associated with pii and a list of all piis \n",
    "# then append each list in one nested list all_pii \n",
    "# so we end up getting something like this:\n",
    "# all_pii = [['N', ['Chris Johnson']], ['O', ['Diamond Star International', 'Diamond Star']], ['P', ['713-832-1234']]]\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"N\")\n",
    "new_list.append(people_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"A\")\n",
    "new_list.append(address_list)      \n",
    "all_pii.append(new_list)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"O\")\n",
    "new_list.append(org_list)      \n",
    "all_pii.append(new_list)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"U\")\n",
    "new_list.append(undefined)      \n",
    "all_pii.append(new_list)\n",
    "\n",
    "# mask all the pii detected by nlp with xxx for testing purposes \n",
    "for entity_group in output:\n",
    "   \n",
    "    if text.find(entity_group[\"word\"]) > -1:\n",
    "        temp = entity_group[\"word\"]\n",
    "        for word in temp.split():       \n",
    "            if text.find(word) > -1:\n",
    "                index = text.find(word)\n",
    "                x = 'x';\n",
    "                for n in range (len(word)-1):\n",
    "                    x = x + 'x';             \n",
    "                text = text[:index] + x + text[index+len(x):]\n",
    "            \n",
    "\n",
    "# The code below will handle all the pii that NLP did not detect\n",
    "#--------------------------------------------------------------------\n",
    "# Extract sentences that have keywords and insert them into new_list \n",
    "\n",
    "new_list = []\n",
    "tok_text = tokenize.sent_tokenize(text2)\n",
    "for sentence in tok_text:\n",
    "    for word in undetected_pii_list:        \n",
    "        if word in sentence:\n",
    "            if sentence not in new_list:\n",
    "                new_list.append(sentence)\n",
    "\n",
    "# get rid of the sentences that have keywords but don't have digits \n",
    "for sentence in new_list:\n",
    "    isDigit = False;\n",
    "    for word in sentence: \n",
    "        if word.isdigit() == True:\n",
    "            isDigit = True;\n",
    "    if isDigit == False:\n",
    "        new_list.remove(sentence)\n",
    "\n",
    "#create a nested list that will hold char of the pii and the value\n",
    "clean_undetected_list = [] \n",
    "\n",
    "for sentence in new_list:\n",
    "    #print(sentence)\n",
    "    sentence_list = []\n",
    "    #check if there is only one type of the document mentioned in the sentence \n",
    "    #if there id more than one, the pii will go into undefined list\n",
    "    one_type = 0;\n",
    "    # assign char depending on the type of pii\n",
    "    for word in id_words:\n",
    "        if word.lower() in sentence.lower():\n",
    "            one_type = one_type + 1\n",
    "            char = \"B\"\n",
    "    for word in dl_words:\n",
    "        if word.lower() in sentence.lower():\n",
    "            one_type = one_type + 1\n",
    "            char = \"D\"\n",
    "    #trace the the numeric pii \n",
    "    for word in sentence.split():\n",
    "        isDigit = False;\n",
    "        for char in word:\n",
    "            if char.isdigit() == True:\n",
    "                isDigit = True; \n",
    "        if isDigit == True and len(word) > 5 and word[0] != \"$\":\n",
    "            # sometimes it will grab pii with punctuation, so we need to make sure to get rid of it before passing into the list \n",
    "            if word[-1] in punc_list:\n",
    "                word = word[:-1]\n",
    "            if one_type == 1:\n",
    "                clean_undetected_list.append(char)\n",
    "                sentence_list.append(word)\n",
    "            else:\n",
    "                undefined.append(word)\n",
    "            #replace pii with xxx\n",
    "            if text.find(word) > -1:\n",
    "                index = text.find(word)\n",
    "                x = 'x';\n",
    "                for n in range (len(word)-1):\n",
    "                    x = x + 'x';             \n",
    "                text = text[:index] + x + text[index+len(x):]\n",
    "    if len(sentence_list) > 0 :\n",
    "        clean_undetected_list.append(sentence_list)\n",
    "#if len(clean_undetected_list) > 0:\n",
    "    #all_pii.append(clean_undetected_list)\n",
    "\n",
    "# detect ssn and replace it with xxx\n",
    "for word in text.split():\n",
    "    #edge case: make sure that there is no punctuation after the ssn; otherwise, it will not be detected\n",
    "    #all the punctuation can not be extracted at this point because \"-\" are part of the ssn\n",
    "    if word[-1] in punc_list:\n",
    "        word = word[:-1]\n",
    "    if (re.match(ssn_validate_pattern, word)):\n",
    "        # append ssn to ssn_list\n",
    "        ssn_list.append(word)\n",
    "        x = 'x';\n",
    "        for n in range (len(word)-1):\n",
    "            x = x + 'x'; \n",
    "        text = text.replace(word, x)\n",
    "        \n",
    "new_list = []\n",
    "new_list.append(\"S\")\n",
    "new_list.append(ssn_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "# detect phone numbers and replace them with xxx       \n",
    "for match in phonenumbers.PhoneNumberMatcher(text, \"US\"):\n",
    "    for word in text.split():\n",
    "        if word == match.raw_string:\n",
    "        #append phone numbers to phone_list\n",
    "            phone_list.append(word)\n",
    "            x = 'x';\n",
    "            for n in range (len(word)-1):\n",
    "                x = x + 'x'; \n",
    "            text = text.replace(word, x)\n",
    "            \n",
    "new_list = []\n",
    "new_list.append(\"P\")\n",
    "new_list.append(phone_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "# detect email and replace it with xxx\n",
    "for word in text.split():\n",
    "    if (re.match(email_validate_pattern, word)):\n",
    "        #edge case: make sure that there is no punctuation after the email; otherwise, it will be grabbed and considered as a part of the email\n",
    "        #all the punctuation can not be extracted at this point because \"@\" and \".\" are part of the email\n",
    "        if word[-1] in punc_list:\n",
    "            word = word[:-1]\n",
    "        email_list.append(word)\n",
    "        x = 'x';\n",
    "        for n in range (len(word)-1):\n",
    "            x = x + 'x'; \n",
    "            \n",
    "        text = text.replace(word, x)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"E\")\n",
    "new_list.append(email_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "# detect ip address and replace it with xxx\n",
    "for word in text.split():\n",
    "    if (re.match(ip_validate_pattern, word) or re.match(mac_validate_pattern, word)):\n",
    "        #edge case: make sure that there is no punctuation after the ip address; otherwise, it will be grabbed and considered as a part of the address\n",
    "        #all the punctuation can not be extracted at this point because \".\" is part of the address \n",
    "        if word[-1] in punc_list:\n",
    "            word = word[:-1]\n",
    "        ip_list.append(word)\n",
    "        x = 'x';\n",
    "        for n in range (len(word)-1):\n",
    "            x = x + 'x'; \n",
    "            \n",
    "        text = text.replace(word, x)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"I\")\n",
    "new_list.append(ip_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "# detect cc and replace it with xxx\n",
    "for word in text.split():\n",
    "    if (re.match(amex_validate_pattern, word) or re.match(visa_validate_pattern, word) or re.match(master_card_validate_pattern, word)):\n",
    "        #edge case: make sure that there is no punctuation after the cc\n",
    "        if word[-1] in punc_list:\n",
    "            word = word[:-1]\n",
    "        cc_list.append(word)\n",
    "        x = 'x';\n",
    "        for n in range (len(word)-1):\n",
    "            x = x + 'x'; \n",
    "            \n",
    "        text = text.replace(word, x)\n",
    "\n",
    "new_list = []\n",
    "new_list.append(\"C\")\n",
    "new_list.append(cc_list)\n",
    "all_pii.append(new_list)\n",
    "\n",
    "#second nlp that we are using for dates \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)            \n",
    " \n",
    "# using spacy library to detect dates\n",
    "for entity in doc.ents:\n",
    "    #print(entity.label_ +\": \" + entity.text)\n",
    "    if entity.label_ == \"DATE\":\n",
    "        temp = entity.text\n",
    "        for punct in punc_list:\n",
    "            if punct in temp:\n",
    "                temp = temp.replace(punct, \" \")\n",
    "        date_list.append(temp)\n",
    "        for word in temp.split():       \n",
    "            if text.find(word) > -1:\n",
    "                index = text.find(word)\n",
    "                x = 'x';\n",
    "                for n in range (len(word)-1):\n",
    "                    x = x + 'x';             \n",
    "                text = text[:index] + x + text[index+len(x):]\n",
    "#print()\n",
    "#print(\"String with masked pii: \")\n",
    "#print(text)\n",
    "new_list = []\n",
    "new_list.append(\"D\")\n",
    "#new_list.append(date_list)\n",
    "\n",
    "# We have to figure out what to do with dates; we will probably just have to manually check if they match the format of the DOB because we don't \n",
    "# need to tokenize all the dates \n",
    "\n",
    "#all_pii.append(new_list)\n",
    "#print(all_pii)\n",
    "\n",
    "token_list = []\n",
    "\n",
    "#sub the pii with uuid to pass it to chat gpt\n",
    "for pii_list in all_pii:\n",
    "    if len(pii_list[1]) != 0:\n",
    "        char = pii_list[0][0]\n",
    "        for element in pii_list[1]:\n",
    "            temp = []\n",
    "            temp.append(char)\n",
    "            temp.append(element)\n",
    "            index = text3.find(element)\n",
    "            str = uuid.uuid4() \n",
    "            temp.append(str.hex)\n",
    "            text3 = text3[:index] + char + \"-\" + str.hex + text3[index+len(element):]\n",
    "            token_list.append(temp)\n",
    "            \n",
    "    \n",
    "print(token_list)            \n",
    "#print(\"String with UUID pii: \")\n",
    "#print(text3)\n",
    "\n",
    "\n",
    "    # TODO: Write to file\n",
    "\n",
    "def replace(text):\n",
    "    # new_text = replace_with_pii(text)\n",
    "    # write to file \n",
    "    pass\n",
    "\n",
    "#Database \n",
    "\n",
    "load_dotenv()\n",
    "SNOWFLAKE_PASSWORD = os.getenv(\"SNOWFLAKE_PASSWORD\")\n",
    "\n",
    "con_def = snowflake.connector.connect(user='BRENDANMORONEY',\n",
    "                                     account='ydpcciy-xn91624',\n",
    "                                     password =SNOWFLAKE_PASSWORD,\n",
    "                                     database='PII_TOKENIZATION',        \n",
    "                                     schema ='PUBLIC',\n",
    "                                     autocommit=True)         \n",
    "\n",
    "db_cursor_def = con_def.cursor()\n",
    "\n",
    "db_cursor_def.execute(\"CREATE WAREHOUSE IF NOT EXISTS pii_warehouse\")\n",
    "db_cursor_def.execute(\"USE WAREHOUSE pii_warehouse\")\n",
    "\n",
    "db_cursor_def.execute(\"CREATE DATABASE IF NOT EXISTS PII_TOKENIZATION\")\n",
    "db_cursor_def.execute(\"USE DATABASE PII_TOKENIZATION\")\n",
    "\n",
    "db_cursor_def.execute(\"CREATE SCHEMA IF NOT EXISTS PUBLIC\")\n",
    "db_cursor_def.execute(\"USE SCHEMA PII_TOKENIZATION.PUBLIC\")\n",
    "\n",
    "#Creates table with PII_value, PII_type and ID\n",
    "\n",
    "db_cursor_def.execute(\"\"\"CREATE OR REPLACE TABLE \n",
    "PII_TOKENIZATION.PUBLIC.PII_Token_XREF (Token TEXT, PII_VALUE \n",
    "VARCHAR(16777216),PII_TYPE VARCHAR(16777216), rec_created_date TIMESTAMP, \n",
    "user_added TEXT, updated_date TIMESTAMP, PRIMARY KEY(Token))\"\"\")\n",
    "\n",
    "#Creates log table\n",
    "\n",
    "db_cursor_def.execute(\"\"\"CREATE OR REPLACE TABLE PII_TOKENIZATION.PUBLIC.log \n",
    "(time TIMESTAMP, user TEXT, document TEXT, PII_type TEXT, override \n",
    "boolean)\"\"\")\n",
    "                      \n",
    "#Practice insert\n",
    "#TODO when PII is matched to Token, add functionality to store and remove from database\n",
    "\n",
    "db_cursor_def.execute(\"INSERT INTO PII_Token_XREF(Token, PII_VALUE, PII_TYPE) VALUES('c2783f59-743e-403c-beac-21cb67076292','Rick Owens', 'N')\")\n",
    "\n",
    "db_cursor_def.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = sys.argv()[1]\n",
    "    remove(text) if sys.argv()[2] == \"1\" else replace(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a00d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47266e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
